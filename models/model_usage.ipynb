{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8faaab6",
   "metadata": {},
   "source": [
    "# How to work with the models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eefb1e",
   "metadata": {},
   "source": [
    "The loading of the model is in *model.py*. The files *arch.py, cdpath_models.py, resnet_ret.py and vision_transformer.py* contain architectures specific to some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff55ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from models.model import *\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310c395",
   "metadata": {},
   "source": [
    "# Select your model #\n",
    "The supported models are listed below. The weight argument links to where to obtain the weights. Some models do not require downloading and are directly accessible through the code (pytorch¹ or huggingface hub²). Some may require to request permission³: \n",
    "- model_name: \"resnet\", num_features: 128, weight: /¹, paper: [*Deep residual learning for Image recognition*](https://arxiv.org/abs/1512.03385)\n",
    "- model_name:  \"deit\", num_features: 128, weight: /¹, paper: [*Training data-efficient image transformers & distillation through attention*](https://arxiv.org/abs/2012.12877)\n",
    "- model_name: \"dino_vit\", num_features: 384, weight: [ImageNet pretrained](https://dl.fbaipublicfiles.com/dino/dino_deitsmall16_pretrain/dino_deitsmall16_linearweights.pth) from [official github](https://github.com/facebookresearch/dino), paper: [*Emerging Properties in Self-Supervised Vision Transformers*](https://arxiv.org/abs/2104.14294)\n",
    "- model_name: \"ibot_vits\", num_features: 384, weight: [ImageNet pretrained](https://lf3-nlp-opensource.bytetos.com/obj/nlp-opensource/archive/2022/ibot/vits_16/checkpoint_teacher.pth) from [official github](https://github.com/bytedance/ibot), paper: [*iBOT: Image BERT Pre-Training with Online Tokenizer*](https://arxiv.org/abs/2111.07832)\n",
    "- model_name: \"byol_light\", num_features: 384, weight: [ImageNet pretrained](https://lightly-ssl-checkpoints.s3.amazonaws.com/imagenet_resnet50_byol_2024-02-14_16-10-09/pretrain/version_0/checkpoints/epoch%3D99-step%3D500400.ckpt) from [lightly](https://github.com/lightly-ai/lightly?tab=readme-ov-file), paper: [*Bootstrap your own latent: A new approach to self-supervised Learning*](https://arxiv.org/abs/2006.07733)\n",
    "- model_name: \"cdpath\", num_features: 512, weight: [Camelyon pretrained](https://nextcloud.centralesupelec.fr/s/S4AZtXSw8Nj2p32/download/CAMELYON17_XY.ckpt) from [official github](https://github.com/jcboyd/cdpath21-gan), paper:  [*Self-Supervised Representation Learning using Visual Field Expansion on Digital Pathology*](https://openaccess.thecvf.com/content/ICCV2021W/CDPath/papers/Boyd_Self-Supervised_Representation_Learning_Using_Visual_Field_Expansion_on_Digital_Pathology_ICCVW_2021_paper.pdf)\n",
    "- model_name: \"ctranspath, num_features: 768, weight: [TCGA pretrained](https://drive.google.com/file/d/1DoDx_70_TLj98gTf6YTXnu4tFhsFocDX/view?usp=sharing) from [official github](https://github.com/Xiyue-Wang/TransPath), paper: [*Transformer-based unsupervised contrastive learning for histopathological image classification*](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002043)\n",
    "- model_name: \"phikon\", num_features: 768, weight: [/²](https://huggingface.co/owkin/phikon), paper: [*Scaling Self-Supervised Learning for Histopathology with Masked Image Modeling*](https://www.medrxiv.org/content/10.1101/2023.07.21.23292757v2)\n",
    "- model_name: \"phikon2\", num_features: 1024, weight: [/²](https://huggingface.co/owkin/phikon-v2), paper: [*Phikon-v2, A large and public feature extractor for biomarker prediction*](https://arxiv.org/abs/2409.09173)\n",
    "- model_name: \"uni\", num_features: 1024, weight: [/²,³](https://github.com/mahmoodlab/UNI), paper: [*Towards a General-Purpose Foundation Model for Computational Pathology*](https://www.nature.com/articles/s41591-024-02857-3)\n",
    "- model_name: \"uni2\", num_features: 1536, weight: [/²,³](https://huggingface.co/MahmoodLab/UNI2-h), paper: / \n",
    "- model_name: \"hoptim\", num_features: 1536, weight: [/²,³](https://huggingface.co/bioptimus/H-optimus-0), software\n",
    "- model_name: \"hoptim1\", num_features: 1536, weight: [/²,³](https://huggingface.co/bioptimus/H-optimus-1), software: [*Hoptimus-1*](https://www.bioptimus.com/h-optimus-1)\n",
    "- model_name: \"ret_ccl\", num_features: 2048, weight: [Paip/TCGA pretrained](https://drive.google.com/drive/folders/1AhstAFVqtTqxeS9WlBpU41BV08LYFUnL?usp=sharing) from [official github](https://github.com/Xiyue-Wang/RetCCL) , paper: [*RetCCL: Clustering-guided contrastive learning for whole-slide image retrieval*](https://www.sciencedirect.com/science/article/abs/pii/S1361841522002730)\n",
    "- model_name: \"virchow2\", num_features: 2560, weight: [/²,³](https://huggingface.co/paige-ai/Virchow2), paper: [*Virchow2: Scaling Self-Supervised Mixed Magnification Models in Pathology*](https://arxiv.org/abs/2408.00738)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a2015",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"select from list\"\n",
    "weight = \"path where to find downloaded weights\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb119407",
   "metadata": {},
   "source": [
    "## Load and encode an image ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ee2bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(model_name, weight, device)\n",
    "\n",
    "query_path = '/path to your image'\n",
    "image = Image.open(query_path).convert('RGB')\n",
    "vec = model.encode(image)\n",
    "print(vec.shape)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
